{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "U-QNHjWTG9Gx"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import platform\n",
        "\n",
        "if os.access('torch_helpers.py', os.F_OK) is False:\n",
        "    if platform.system() == 'Windows':\n",
        "        !curl !wget https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py > torch_helpers.py\n",
        "    else:\n",
        "        !wget torch_helpers.py https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUbWkfSNG_kL",
        "outputId": "4e8afbf4-6ea5-448a-cd75-80195e55f728"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-11-12 04:34:04--  http://torch_helpers.py/\n",
            "Resolving torch_helpers.py (torch_helpers.py)... failed: Name or service not known.\n",
            "wget: unable to resolve host address ‘torch_helpers.py’\n",
            "--2022-11-12 04:34:04--  https://raw.githubusercontent.com/FIUBA-Posgrado-Inteligencia-Artificial/procesamiento_lenguaje_natural/main/scripts/torch_helpers.py\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.108.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 23883 (23K) [text/plain]\n",
            "Saving to: ‘torch_helpers.py’\n",
            "\n",
            "\rtorch_helpers.py      0%[                    ]       0  --.-KB/s               \rtorch_helpers.py    100%[===================>]  23.32K  --.-KB/s    in 0.002s  \n",
            "\n",
            "2022-11-12 04:34:04 (13.2 MB/s) - ‘torch_helpers.py’ saved [23883/23883]\n",
            "\n",
            "FINISHED --2022-11-12 04:34:04--\n",
            "Total wall clock time: 0.2s\n",
            "Downloaded: 1 files, 23K in 0.002s (13.2 MB/s)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchinfo\n",
        "from torchinfo import summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xwtRsYiHrPl",
        "outputId": "d55fe1ee-b289-472a-99d4-cb5337e3e0c7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchinfo\n",
            "  Downloading torchinfo-1.7.1-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: torchinfo\n",
            "Successfully installed torchinfo-1.7.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = [[0.00, 0.010, 0.019]]\n",
        "for i in range(20):\n",
        "    rand = round(np.random.uniform(0, 0.05), 3)\n",
        "    t = round(X[i][2] + rand, 3)\n",
        "    r = list([X[i][1], X[i][2], t])\n",
        "    X.append(r)"
      ],
      "metadata": {
        "id": "0Y_pCDZTG_p4"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = [X[i + 1][2] for i in range(20)]\n",
        "X = X[:-1]"
      ],
      "metadata": {
        "id": "0XvQRdQmG_r3"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c030mKigHJt_",
        "outputId": "9d381cc7-8998-42c7-dcd1-5cfa0aba6c8f"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[[0.0, 0.01, 0.019],\n",
              " [0.01, 0.019, 0.039],\n",
              " [0.019, 0.039, 0.065],\n",
              " [0.039, 0.065, 0.073],\n",
              " [0.065, 0.073, 0.104],\n",
              " [0.073, 0.104, 0.119],\n",
              " [0.104, 0.119, 0.122],\n",
              " [0.119, 0.122, 0.172],\n",
              " [0.122, 0.172, 0.179],\n",
              " [0.172, 0.179, 0.212],\n",
              " [0.179, 0.212, 0.217],\n",
              " [0.212, 0.217, 0.231],\n",
              " [0.217, 0.231, 0.245],\n",
              " [0.231, 0.245, 0.281],\n",
              " [0.245, 0.281, 0.329],\n",
              " [0.281, 0.329, 0.365],\n",
              " [0.329, 0.365, 0.381],\n",
              " [0.365, 0.381, 0.399],\n",
              " [0.381, 0.399, 0.413],\n",
              " [0.399, 0.413, 0.421]]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_loader, valid_loader, optimizer, criterion, epochs=100):\n",
        "    # Defino listas para realizar graficas de los resultados\n",
        "    train_loss = []\n",
        "    valid_loss = []\n",
        "\n",
        "    # Defino mi loop de entrenamiento\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        epoch_train_loss = 0.0\n",
        "        epoch_train_accuracy = 0.0\n",
        "\n",
        "        for train_data, train_target in train_loader:\n",
        "\n",
        "            # Seteo los gradientes en cero ya que, por defecto, PyTorch\n",
        "            # los va acumulando\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(train_data)\n",
        "\n",
        "            # Computo el error de la salida comparando contra las etiquetas\n",
        "            loss = criterion(output, train_target)\n",
        "\n",
        "            # Almaceno el error del batch para luego tener el error promedio de la epoca\n",
        "            epoch_train_loss += loss.item()\n",
        "\n",
        "            # Computo el nuevo set de gradientes a lo largo de toda la red\n",
        "            loss.backward()\n",
        "\n",
        "            # Realizo el paso de optimizacion actualizando los parametros de toda la red\n",
        "            optimizer.step()\n",
        "\n",
        "        # Calculo la media de error para la epoca de entrenamiento.\n",
        "        # La longitud de train_loader es igual a la cantidad de batches dentro de una epoca.\n",
        "        epoch_train_loss = epoch_train_loss / len(train_loader)\n",
        "        train_loss.append(epoch_train_loss)\n",
        "\n",
        "        # Realizo el paso de validación computando error y accuracy, y\n",
        "        # almacenando los valores para imprimirlos y graficarlos\n",
        "        valid_data, valid_target = iter(valid_loader).next()\n",
        "        output = model(valid_data)\n",
        "        \n",
        "        epoch_valid_loss = criterion(output, valid_target).item()\n",
        "        valid_loss.append(epoch_valid_loss)\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}/{epochs} - Train loss {epoch_train_loss:.3f} - Valid Loss {epoch_valid_loss:.3f}\")\n",
        "\n",
        "    history = {\n",
        "        \"loss\": train_loss,\n",
        "        \"val_loss\": valid_loss,\n",
        "    }\n",
        "    return history"
      ],
      "metadata": {
        "id": "09Jz7QY1HJw2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cada dato X lo transformarmos en una matriz de 1 fila 1 columna (1x1)\n",
        "X = np.array(X).reshape(len(X), len(X[0]), 1)\n",
        "print(\"datos X:\", X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAYpRgmkHODG",
        "outputId": "0baebf4e-ed39-4250-eb22-1987d846cb66"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datos X: [[[0.   ]\n",
            "  [0.01 ]\n",
            "  [0.019]]\n",
            "\n",
            " [[0.01 ]\n",
            "  [0.019]\n",
            "  [0.039]]\n",
            "\n",
            " [[0.019]\n",
            "  [0.039]\n",
            "  [0.065]]\n",
            "\n",
            " [[0.039]\n",
            "  [0.065]\n",
            "  [0.073]]\n",
            "\n",
            " [[0.065]\n",
            "  [0.073]\n",
            "  [0.104]]\n",
            "\n",
            " [[0.073]\n",
            "  [0.104]\n",
            "  [0.119]]\n",
            "\n",
            " [[0.104]\n",
            "  [0.119]\n",
            "  [0.122]]\n",
            "\n",
            " [[0.119]\n",
            "  [0.122]\n",
            "  [0.172]]\n",
            "\n",
            " [[0.122]\n",
            "  [0.172]\n",
            "  [0.179]]\n",
            "\n",
            " [[0.172]\n",
            "  [0.179]\n",
            "  [0.212]]\n",
            "\n",
            " [[0.179]\n",
            "  [0.212]\n",
            "  [0.217]]\n",
            "\n",
            " [[0.212]\n",
            "  [0.217]\n",
            "  [0.231]]\n",
            "\n",
            " [[0.217]\n",
            "  [0.231]\n",
            "  [0.245]]\n",
            "\n",
            " [[0.231]\n",
            "  [0.245]\n",
            "  [0.281]]\n",
            "\n",
            " [[0.245]\n",
            "  [0.281]\n",
            "  [0.329]]\n",
            "\n",
            " [[0.281]\n",
            "  [0.329]\n",
            "  [0.365]]\n",
            "\n",
            " [[0.329]\n",
            "  [0.365]\n",
            "  [0.381]]\n",
            "\n",
            " [[0.365]\n",
            "  [0.381]\n",
            "  [0.399]]\n",
            "\n",
            " [[0.381]\n",
            "  [0.399]\n",
            "  [0.413]]\n",
            "\n",
            " [[0.399]\n",
            "  [0.413]\n",
            "  [0.421]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (batch size, seq_len, input_size)\n",
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qE3uI0UsHOFt",
        "outputId": "6dc9149a-69bf-46da-875c-7f9fa2f1be42"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 3, 1)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.asanyarray(y)\n",
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlEjaMYcHOH6",
        "outputId": "5c35e18b-7bf8-4346-fba1-fe3f3308b415"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20,)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Data(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        # Convertir los arrays de numpy a tensores. \n",
        "        # pytorch espera en general entradas 32bits\n",
        "        self.x = torch.from_numpy(x.astype(np.float32))\n",
        "        # las loss unfction esperan la salida float\n",
        "        self.y = torch.from_numpy(y.astype(np.float32)).float().view(-1, 1)\n",
        "\n",
        "        self.len = self.y.shape[0]\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return self.x[index], self.y[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "data_set = Data(X, y)\n",
        "\n",
        "input_dim = data_set.x.shape[1:]\n",
        "seq_length = input_dim[0]\n",
        "input_size = input_dim[1]\n",
        "print(\"Input dim\", input_dim)\n",
        "print(\"seq_length:\", seq_length)\n",
        "print(\"input_size:\", input_size)\n",
        "\n",
        "output_dim = data_set.y.shape[1]\n",
        "print(\"Output dim\", output_dim)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwrXg-57HJzV",
        "outputId": "e6d3f518-3309-4fb7-9052-ae7f3bae8f7a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input dim torch.Size([3, 1])\n",
            "seq_length: 3\n",
            "input_size: 1\n",
            "Output dim 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(data_set))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6hatVm3PbKn",
        "outputId": "f6e68012-1807-41f0-fded-64ec4b2d3c27"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0100],\n",
              "         [0.0190]]), tensor([0.0390]))"
            ]
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDfAcGJQPmSs",
        "outputId": "7911234c-bc98-4b0a-cd15-7be38e50e079"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.039"
            ]
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "valid_set_size = int(data_set.len * 0.2)\n",
        "train_set_size = data_set.len - valid_set_size\n",
        "\n",
        "# Cuando trabajmos con una serie temporal no mezclamos (shuffle) los datos\n",
        "train_set = torch.utils.data.Subset(data_set, range(train_set_size))\n",
        "valid_set = torch.utils.data.Subset(data_set, range(train_set_size, data_set.len))\n",
        "\n",
        "print(\"Tamaño del conjunto de entrenamiento:\", len(train_set))\n",
        "print(\"Tamaño del conjunto de validacion:\", len(valid_set))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_set, batch_size=len(train_set), shuffle=False)\n",
        "valid_loader = torch.utils.data.DataLoader(valid_set, batch_size=len(valid_set), shuffle=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SDOXLFpKHJ1M",
        "outputId": "9c33b314-c16a-471c-d3af-d55c8944be73"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño del conjunto de entrenamiento: 16\n",
            "Tamaño del conjunto de validacion: 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch_helpers import CustomLSTM\n",
        "\n",
        "class Model1(nn.Module):\n",
        "    def __init__(self, input_size, output_dim):\n",
        "        super().__init__()\n",
        "        self.lstm1 = CustomLSTM(input_size=input_size, hidden_size=64, activation=nn.ReLU()) # LSTM layer\n",
        "        self.fc = nn.Linear(in_features=64, out_features=output_dim) #  # Fully connected layer\n",
        "        \n",
        "    def forward(self, x):\n",
        "        lstm_output, _ = self.lstm1(x)\n",
        "        out = self.fc(lstm_output[:,-1,:]) # take last output (last seq)\n",
        "        return out\n",
        "\n",
        "model1 = Model1(input_size=input_size, output_dim=output_dim)\n",
        "\n",
        "# Crear el optimizador la una función de error\n",
        "model1_optimizer = torch.optim.Adam(model1.parameters(), lr=0.001)\n",
        "model1_criterion = nn.MSELoss()  # mean squared error\n",
        "\n",
        "summary(model1, input_size=(1, seq_length, input_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrGB5zPcHVry",
        "outputId": "2b78e89c-7e58-452e-e2a4-9d52c10a3f1f"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "==========================================================================================\n",
              "Layer (type:depth-idx)                   Output Shape              Param #\n",
              "==========================================================================================\n",
              "Model1                                   [1, 1]                    --\n",
              "├─CustomLSTM: 1-1                        [1, 3, 64]                16,896\n",
              "│    └─Sigmoid: 2-1                      [1, 64]                   --\n",
              "│    └─Sigmoid: 2-2                      [1, 64]                   --\n",
              "│    └─ReLU: 2-3                         [1, 64]                   --\n",
              "│    └─Sigmoid: 2-4                      [1, 64]                   --\n",
              "│    └─ReLU: 2-5                         [1, 64]                   --\n",
              "│    └─Sigmoid: 2-6                      [1, 64]                   --\n",
              "│    └─Sigmoid: 2-7                      [1, 64]                   --\n",
              "│    └─ReLU: 2-8                         [1, 64]                   --\n",
              "│    └─Sigmoid: 2-9                      [1, 64]                   --\n",
              "│    └─ReLU: 2-10                        [1, 64]                   --\n",
              "│    └─Sigmoid: 2-11                     [1, 64]                   --\n",
              "│    └─Sigmoid: 2-12                     [1, 64]                   --\n",
              "│    └─ReLU: 2-13                        [1, 64]                   --\n",
              "│    └─Sigmoid: 2-14                     [1, 64]                   --\n",
              "│    └─ReLU: 2-15                        [1, 64]                   --\n",
              "├─Linear: 1-2                            [1, 1]                    65\n",
              "==========================================================================================\n",
              "Total params: 16,961\n",
              "Trainable params: 16,961\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (M): 0.00\n",
              "==========================================================================================\n",
              "Input size (MB): 0.00\n",
              "Forward/backward pass size (MB): 0.00\n",
              "Params size (MB): 0.00\n",
              "Estimated Total Size (MB): 0.00\n",
              "=========================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history1 = train(model1,\n",
        "                train_loader,\n",
        "                valid_loader,\n",
        "                model1_optimizer,\n",
        "                model1_criterion,\n",
        "                epochs=500\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iux3zYqmHtlX",
        "outputId": "29cf9f9d-2828-4bc3-d73c-c7747630314c"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1/500 - Train loss 0.019 - Valid Loss 0.097\n",
            "Epoch: 2/500 - Train loss 0.019 - Valid Loss 0.094\n",
            "Epoch: 3/500 - Train loss 0.018 - Valid Loss 0.092\n",
            "Epoch: 4/500 - Train loss 0.017 - Valid Loss 0.090\n",
            "Epoch: 5/500 - Train loss 0.017 - Valid Loss 0.088\n",
            "Epoch: 6/500 - Train loss 0.016 - Valid Loss 0.086\n",
            "Epoch: 7/500 - Train loss 0.016 - Valid Loss 0.084\n",
            "Epoch: 8/500 - Train loss 0.015 - Valid Loss 0.082\n",
            "Epoch: 9/500 - Train loss 0.015 - Valid Loss 0.080\n",
            "Epoch: 10/500 - Train loss 0.014 - Valid Loss 0.078\n",
            "Epoch: 11/500 - Train loss 0.014 - Valid Loss 0.076\n",
            "Epoch: 12/500 - Train loss 0.013 - Valid Loss 0.074\n",
            "Epoch: 13/500 - Train loss 0.013 - Valid Loss 0.072\n",
            "Epoch: 14/500 - Train loss 0.013 - Valid Loss 0.070\n",
            "Epoch: 15/500 - Train loss 0.012 - Valid Loss 0.068\n",
            "Epoch: 16/500 - Train loss 0.012 - Valid Loss 0.067\n",
            "Epoch: 17/500 - Train loss 0.012 - Valid Loss 0.065\n",
            "Epoch: 18/500 - Train loss 0.011 - Valid Loss 0.063\n",
            "Epoch: 19/500 - Train loss 0.011 - Valid Loss 0.062\n",
            "Epoch: 20/500 - Train loss 0.011 - Valid Loss 0.060\n",
            "Epoch: 21/500 - Train loss 0.011 - Valid Loss 0.059\n",
            "Epoch: 22/500 - Train loss 0.011 - Valid Loss 0.057\n",
            "Epoch: 23/500 - Train loss 0.010 - Valid Loss 0.056\n",
            "Epoch: 24/500 - Train loss 0.010 - Valid Loss 0.054\n",
            "Epoch: 25/500 - Train loss 0.010 - Valid Loss 0.053\n",
            "Epoch: 26/500 - Train loss 0.010 - Valid Loss 0.051\n",
            "Epoch: 27/500 - Train loss 0.010 - Valid Loss 0.050\n",
            "Epoch: 28/500 - Train loss 0.010 - Valid Loss 0.049\n",
            "Epoch: 29/500 - Train loss 0.010 - Valid Loss 0.048\n",
            "Epoch: 30/500 - Train loss 0.009 - Valid Loss 0.047\n",
            "Epoch: 31/500 - Train loss 0.009 - Valid Loss 0.046\n",
            "Epoch: 32/500 - Train loss 0.009 - Valid Loss 0.045\n",
            "Epoch: 33/500 - Train loss 0.009 - Valid Loss 0.044\n",
            "Epoch: 34/500 - Train loss 0.009 - Valid Loss 0.043\n",
            "Epoch: 35/500 - Train loss 0.009 - Valid Loss 0.042\n",
            "Epoch: 36/500 - Train loss 0.009 - Valid Loss 0.041\n",
            "Epoch: 37/500 - Train loss 0.009 - Valid Loss 0.041\n",
            "Epoch: 38/500 - Train loss 0.009 - Valid Loss 0.040\n",
            "Epoch: 39/500 - Train loss 0.009 - Valid Loss 0.039\n",
            "Epoch: 40/500 - Train loss 0.009 - Valid Loss 0.039\n",
            "Epoch: 41/500 - Train loss 0.009 - Valid Loss 0.038\n",
            "Epoch: 42/500 - Train loss 0.009 - Valid Loss 0.038\n",
            "Epoch: 43/500 - Train loss 0.009 - Valid Loss 0.037\n",
            "Epoch: 44/500 - Train loss 0.009 - Valid Loss 0.037\n",
            "Epoch: 45/500 - Train loss 0.009 - Valid Loss 0.037\n",
            "Epoch: 46/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 47/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 48/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 49/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 50/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 51/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 52/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 53/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 54/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 55/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 56/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 57/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 58/500 - Train loss 0.009 - Valid Loss 0.036\n",
            "Epoch: 59/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 60/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 61/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 62/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 63/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 64/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 65/500 - Train loss 0.008 - Valid Loss 0.036\n",
            "Epoch: 66/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 67/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 68/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 69/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 70/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 71/500 - Train loss 0.008 - Valid Loss 0.035\n",
            "Epoch: 72/500 - Train loss 0.008 - Valid Loss 0.034\n",
            "Epoch: 73/500 - Train loss 0.008 - Valid Loss 0.034\n",
            "Epoch: 74/500 - Train loss 0.008 - Valid Loss 0.034\n",
            "Epoch: 75/500 - Train loss 0.008 - Valid Loss 0.034\n",
            "Epoch: 76/500 - Train loss 0.008 - Valid Loss 0.033\n",
            "Epoch: 77/500 - Train loss 0.008 - Valid Loss 0.033\n",
            "Epoch: 78/500 - Train loss 0.008 - Valid Loss 0.033\n",
            "Epoch: 79/500 - Train loss 0.008 - Valid Loss 0.032\n",
            "Epoch: 80/500 - Train loss 0.008 - Valid Loss 0.032\n",
            "Epoch: 81/500 - Train loss 0.008 - Valid Loss 0.032\n",
            "Epoch: 82/500 - Train loss 0.008 - Valid Loss 0.031\n",
            "Epoch: 83/500 - Train loss 0.007 - Valid Loss 0.031\n",
            "Epoch: 84/500 - Train loss 0.007 - Valid Loss 0.031\n",
            "Epoch: 85/500 - Train loss 0.007 - Valid Loss 0.030\n",
            "Epoch: 86/500 - Train loss 0.007 - Valid Loss 0.030\n",
            "Epoch: 87/500 - Train loss 0.007 - Valid Loss 0.030\n",
            "Epoch: 88/500 - Train loss 0.007 - Valid Loss 0.029\n",
            "Epoch: 89/500 - Train loss 0.007 - Valid Loss 0.029\n",
            "Epoch: 90/500 - Train loss 0.007 - Valid Loss 0.029\n",
            "Epoch: 91/500 - Train loss 0.007 - Valid Loss 0.028\n",
            "Epoch: 92/500 - Train loss 0.007 - Valid Loss 0.028\n",
            "Epoch: 93/500 - Train loss 0.007 - Valid Loss 0.027\n",
            "Epoch: 94/500 - Train loss 0.007 - Valid Loss 0.027\n",
            "Epoch: 95/500 - Train loss 0.007 - Valid Loss 0.027\n",
            "Epoch: 96/500 - Train loss 0.007 - Valid Loss 0.026\n",
            "Epoch: 97/500 - Train loss 0.007 - Valid Loss 0.026\n",
            "Epoch: 98/500 - Train loss 0.007 - Valid Loss 0.026\n",
            "Epoch: 99/500 - Train loss 0.006 - Valid Loss 0.025\n",
            "Epoch: 100/500 - Train loss 0.006 - Valid Loss 0.025\n",
            "Epoch: 101/500 - Train loss 0.006 - Valid Loss 0.025\n",
            "Epoch: 102/500 - Train loss 0.006 - Valid Loss 0.024\n",
            "Epoch: 103/500 - Train loss 0.006 - Valid Loss 0.024\n",
            "Epoch: 104/500 - Train loss 0.006 - Valid Loss 0.023\n",
            "Epoch: 105/500 - Train loss 0.006 - Valid Loss 0.023\n",
            "Epoch: 106/500 - Train loss 0.006 - Valid Loss 0.022\n",
            "Epoch: 107/500 - Train loss 0.006 - Valid Loss 0.022\n",
            "Epoch: 108/500 - Train loss 0.006 - Valid Loss 0.022\n",
            "Epoch: 109/500 - Train loss 0.006 - Valid Loss 0.021\n",
            "Epoch: 110/500 - Train loss 0.006 - Valid Loss 0.021\n",
            "Epoch: 111/500 - Train loss 0.006 - Valid Loss 0.020\n",
            "Epoch: 112/500 - Train loss 0.005 - Valid Loss 0.020\n",
            "Epoch: 113/500 - Train loss 0.005 - Valid Loss 0.019\n",
            "Epoch: 114/500 - Train loss 0.005 - Valid Loss 0.019\n",
            "Epoch: 115/500 - Train loss 0.005 - Valid Loss 0.018\n",
            "Epoch: 116/500 - Train loss 0.005 - Valid Loss 0.018\n",
            "Epoch: 117/500 - Train loss 0.005 - Valid Loss 0.017\n",
            "Epoch: 118/500 - Train loss 0.005 - Valid Loss 0.017\n",
            "Epoch: 119/500 - Train loss 0.005 - Valid Loss 0.016\n",
            "Epoch: 120/500 - Train loss 0.005 - Valid Loss 0.016\n",
            "Epoch: 121/500 - Train loss 0.005 - Valid Loss 0.015\n",
            "Epoch: 122/500 - Train loss 0.004 - Valid Loss 0.015\n",
            "Epoch: 123/500 - Train loss 0.004 - Valid Loss 0.014\n",
            "Epoch: 124/500 - Train loss 0.004 - Valid Loss 0.014\n",
            "Epoch: 125/500 - Train loss 0.004 - Valid Loss 0.013\n",
            "Epoch: 126/500 - Train loss 0.004 - Valid Loss 0.013\n",
            "Epoch: 127/500 - Train loss 0.004 - Valid Loss 0.012\n",
            "Epoch: 128/500 - Train loss 0.004 - Valid Loss 0.012\n",
            "Epoch: 129/500 - Train loss 0.004 - Valid Loss 0.011\n",
            "Epoch: 130/500 - Train loss 0.004 - Valid Loss 0.010\n",
            "Epoch: 131/500 - Train loss 0.003 - Valid Loss 0.010\n",
            "Epoch: 132/500 - Train loss 0.003 - Valid Loss 0.009\n",
            "Epoch: 133/500 - Train loss 0.003 - Valid Loss 0.009\n",
            "Epoch: 134/500 - Train loss 0.003 - Valid Loss 0.008\n",
            "Epoch: 135/500 - Train loss 0.003 - Valid Loss 0.008\n",
            "Epoch: 136/500 - Train loss 0.003 - Valid Loss 0.007\n",
            "Epoch: 137/500 - Train loss 0.003 - Valid Loss 0.007\n",
            "Epoch: 138/500 - Train loss 0.003 - Valid Loss 0.006\n",
            "Epoch: 139/500 - Train loss 0.003 - Valid Loss 0.006\n",
            "Epoch: 140/500 - Train loss 0.002 - Valid Loss 0.005\n",
            "Epoch: 141/500 - Train loss 0.002 - Valid Loss 0.005\n",
            "Epoch: 142/500 - Train loss 0.002 - Valid Loss 0.004\n",
            "Epoch: 143/500 - Train loss 0.002 - Valid Loss 0.004\n",
            "Epoch: 144/500 - Train loss 0.002 - Valid Loss 0.004\n",
            "Epoch: 145/500 - Train loss 0.002 - Valid Loss 0.003\n",
            "Epoch: 146/500 - Train loss 0.002 - Valid Loss 0.003\n",
            "Epoch: 147/500 - Train loss 0.002 - Valid Loss 0.002\n",
            "Epoch: 148/500 - Train loss 0.002 - Valid Loss 0.002\n",
            "Epoch: 149/500 - Train loss 0.001 - Valid Loss 0.002\n",
            "Epoch: 150/500 - Train loss 0.001 - Valid Loss 0.001\n",
            "Epoch: 151/500 - Train loss 0.001 - Valid Loss 0.001\n",
            "Epoch: 152/500 - Train loss 0.001 - Valid Loss 0.001\n",
            "Epoch: 153/500 - Train loss 0.001 - Valid Loss 0.001\n",
            "Epoch: 154/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 155/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 156/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 157/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 158/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 159/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 160/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 161/500 - Train loss 0.001 - Valid Loss 0.000\n",
            "Epoch: 162/500 - Train loss 0.000 - Valid Loss 0.000\n",
            "Epoch: 163/500 - Train loss 0.000 - Valid Loss 0.000\n",
            "Epoch: 164/500 - Train loss 0.000 - Valid Loss 0.000\n",
            "Epoch: 165/500 - Train loss 0.000 - Valid Loss 0.000\n",
            "Epoch: 166/500 - Train loss 0.000 - Valid Loss 0.001\n",
            "Epoch: 167/500 - Train loss 0.000 - Valid Loss 0.001\n",
            "Epoch: 168/500 - Train loss 0.000 - Valid Loss 0.001\n",
            "Epoch: 169/500 - Train loss 0.000 - Valid Loss 0.001\n",
            "Epoch: 170/500 - Train loss 0.000 - Valid Loss 0.001\n",
            "Epoch: 171/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 172/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 173/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 174/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 175/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 176/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 177/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 178/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 179/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 180/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 181/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 182/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 183/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 184/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 185/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 186/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 187/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 188/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 189/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 190/500 - Train loss 0.000 - Valid Loss 0.004\n",
            "Epoch: 191/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 192/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 193/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 194/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 195/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 196/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 197/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 198/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 199/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 200/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 201/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 202/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 203/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 204/500 - Train loss 0.000 - Valid Loss 0.003\n",
            "Epoch: 205/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 206/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 207/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 208/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 209/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 210/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 211/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 212/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 213/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 214/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 215/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 216/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 217/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 218/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 219/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 220/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 221/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 222/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 223/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 224/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 225/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 226/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 227/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 228/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 229/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 230/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 231/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 232/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 233/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 234/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 235/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 236/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 237/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 238/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 239/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 240/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 241/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 242/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 243/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 244/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 245/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 246/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 247/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 248/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 249/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 250/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 251/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 252/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 253/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 254/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 255/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 256/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 257/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 258/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 259/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 260/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 261/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 262/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 263/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 264/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 265/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 266/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 267/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 268/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 269/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 270/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 271/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 272/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 273/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 274/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 275/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 276/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 277/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 278/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 279/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 280/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 281/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 282/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 283/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 284/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 285/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 286/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 287/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 288/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 289/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 290/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 291/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 292/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 293/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 294/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 295/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 296/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 297/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 298/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 299/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 300/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 301/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 302/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 303/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 304/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 305/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 306/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 307/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 308/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 309/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 310/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 311/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 312/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 313/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 314/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 315/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 316/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 317/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 318/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 319/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 320/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 321/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 322/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 323/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 324/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 325/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 326/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 327/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 328/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 329/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 330/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 331/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 332/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 333/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 334/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 335/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 336/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 337/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 338/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 339/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 340/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 341/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 342/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 343/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 344/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 345/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 346/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 347/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 348/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 349/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 350/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 351/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 352/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 353/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 354/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 355/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 356/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 357/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 358/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 359/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 360/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 361/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 362/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 363/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 364/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 365/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 366/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 367/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 368/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 369/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 370/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 371/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 372/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 373/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 374/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 375/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 376/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 377/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 378/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 379/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 380/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 381/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 382/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 383/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 384/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 385/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 386/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 387/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 388/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 389/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 390/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 391/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 392/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 393/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 394/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 395/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 396/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 397/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 398/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 399/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 400/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 401/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 402/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 403/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 404/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 405/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 406/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 407/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 408/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 409/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 410/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 411/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 412/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 413/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 414/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 415/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 416/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 417/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 418/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 419/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 420/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 421/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 422/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 423/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 424/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 425/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 426/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 427/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 428/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 429/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 430/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 431/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 432/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 433/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 434/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 435/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 436/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 437/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 438/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 439/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 440/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 441/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 442/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 443/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 444/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 445/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 446/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 447/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 448/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 449/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 450/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 451/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 452/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 453/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 454/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 455/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 456/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 457/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 458/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 459/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 460/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 461/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 462/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 463/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 464/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 465/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 466/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 467/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 468/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 469/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 470/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 471/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 472/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 473/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 474/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 475/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 476/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 477/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 478/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 479/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 480/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 481/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 482/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 483/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 484/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 485/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 486/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 487/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 488/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 489/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 490/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 491/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 492/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 493/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 494/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 495/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 496/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 497/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 498/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 499/500 - Train loss 0.000 - Valid Loss 0.002\n",
            "Epoch: 500/500 - Train loss 0.000 - Valid Loss 0.002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_count = range(1, len(history1['loss']) + 1)\n",
        "sns.lineplot(x=epoch_count,  y=history1['loss'], label='train')\n",
        "sns.lineplot(x=epoch_count,  y=history1['val_loss'], label='valid')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "0CGGYxnzHXRR",
        "outputId": "036ca199-7666-4221-c0b9-288dced780ad"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensayo\n",
        "x_test = [y[-3], y[-2], y[-1]]\n",
        "preds = []\n",
        "for i in range(10):\n",
        "\n",
        "  if i != 0:\n",
        "      x_test = [x_test[-2], x_test[-1], preds[-1]]\n",
        "\n",
        "  test_input = np.array(x_test)\n",
        "  test_input = test_input.reshape((1, seq_length, input_size))\n",
        "  test_input = torch.from_numpy(test_input.astype(np.float32))\n",
        "\n",
        "  y_hat = model1(test_input).detach().numpy()\n",
        "  preds.append(y_hat)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWEfh9QKH49t",
        "outputId": "f2f8dbe0-f966-44a6-beb6-b925d9205e39"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9qWzK5GXQMce",
        "outputId": "d46b6543-4436-43a2-824b-3e4b1ef54052"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.5410771]], dtype=float32),\n",
              " array([[0.6065118]], dtype=float32),\n",
              " array([[0.70872545]], dtype=float32),\n",
              " array([[0.8687689]], dtype=float32),\n",
              " array([[1.0877287]], dtype=float32),\n",
              " array([[1.4591755]], dtype=float32),\n",
              " array([[2.1581287]], dtype=float32),\n",
              " array([[3.686888]], dtype=float32),\n",
              " array([[7.7305684]], dtype=float32),\n",
              " array([[19.09575]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-K1ixesQRHy1",
        "outputId": "363fa837-5300-4ed9-f24e-b8226c88c899"
      },
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([[0.50389487]], dtype=float32),\n",
              " array([[0.5295889]], dtype=float32),\n",
              " array([[0.56736445]], dtype=float32),\n",
              " array([[0.6153339]], dtype=float32),\n",
              " array([[0.6547185]], dtype=float32),\n",
              " array([[0.70270264]], dtype=float32),\n",
              " array([[0.75568086]], dtype=float32),\n",
              " array([[0.8117021]], dtype=float32),\n",
              " array([[0.8784789]], dtype=float32),\n",
              " array([[0.95683193]], dtype=float32)]"
            ]
          },
          "metadata": {},
          "execution_count": 111
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx = 18\n",
        "x_test = X[idx]\n",
        "x_test\n",
        "\n",
        "test_input = np.array(x_test)\n",
        "test_input = test_input.reshape((1, seq_length, input_size))\n",
        "test_input = torch.from_numpy(test_input.astype(np.float32))\n",
        "\n",
        "y_hat = model1(test_input)\n",
        "print(y_hat)\n",
        "print(y[idx])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8P31U1oVMVXY",
        "outputId": "d678c3e2-29be-47ac-e103-038fe21fab10"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4990]], grad_fn=<AddmmBackward0>)\n",
            "0.421\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46t-4huNM9T6",
        "outputId": "82ed8507-d6c7-45ec-97c6-d0273d464efe"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.039, 0.065, 0.073, 0.104, 0.119, 0.122, 0.172, 0.179, 0.212,\n",
              "       0.217, 0.231, 0.245, 0.281, 0.329, 0.365, 0.381, 0.399, 0.413,\n",
              "       0.421, 0.444])"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X[5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V-jq7DbYNpPQ",
        "outputId": "7b46bea6-2322-4c7f-8714-7aa0ce6e8ca0"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.073],\n",
              "       [0.104],\n",
              "       [0.119]])"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MArU_lLzNlcg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_test.detach().numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "TtYL6bm_Lqya",
        "outputId": "41c99bc7-eb97-4147-a18f-6ee30da8f6c7"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-51-bb452257f277>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'detach'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "kBDSzAewHVuS"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "mes = np.arange(0, 30, 1)\n",
        "old_out = np.concatenate((y, np.full((1, 10), y[-1])), axis = None)\n",
        "new_out = np.concatenate((y, preds), axis = None)\n",
        "plt.plot(mes, new_out)\n",
        "plt.plot(mes, old_out)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "ZdDLBHRAISNB",
        "outputId": "a3569027-556b-4b3e-d161-0086afd08369"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.full((1, 10), y[-1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cftWWoO1HVwa",
        "outputId": "b47b0427-062f-40c8-d49a-c1554b156e74"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.444, 0.444, 0.444, 0.444, 0.444, 0.444, 0.444, 0.444, 0.444,\n",
              "        0.444]])"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y[1:] - y[:-1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1JCCljhHVyT",
        "outputId": "0e871a8a-aaac-4d5c-ba57-ae86a9b3379a"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.007, 0.057, 0.009, 0.024, 0.058, 0.049, 0.047, 0.017, 0.074,\n",
              "       0.065, 0.064, 0.049, 0.062, 0.044, 0.063, 0.057, 0.075, 0.044,\n",
              "       0.065])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E6970CJMHV0P"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "42PzkP6-HV2c"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}